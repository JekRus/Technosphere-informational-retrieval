{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import base64\n",
    "import csv\n",
    "import gzip\n",
    "import zlib\n",
    "import logging\n",
    "import re\n",
    "import urllib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "from collections import namedtuple, Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy import stats\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRACE_NUM = 1000\n",
    "reload(logging)\n",
    "logging.basicConfig(format='%(asctime)s %(levelname)s:%(message)s', level=logging.INFO, datefmt='%H:%M:%S')\n",
    "\n",
    "def trace(items_num, trace_num=TRACE_NUM):\n",
    "    if items_num % trace_num == 0: logging.info(\"Complete items %05d\" % items_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_utf8(text):\n",
    "    if isinstance(text, unicode): text = text.encode('utf8')\n",
    "    return text\n",
    "\n",
    "def convert2unicode(f):\n",
    "    def tmp(text):\n",
    "        if not isinstance(text, unicode): text = text.decode('utf8')\n",
    "        return f(text)\n",
    "    return tmp\n",
    "\n",
    "def convert2lower(f):\n",
    "    def tmp(text):        \n",
    "        return f(text.lower())\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self._text = []\n",
    "        self._title = \"\"\n",
    "        self._in_title = False\n",
    "        self._title_text = []\n",
    "        self._anchor = False\n",
    "        self._anchor_text = []\n",
    "        self._head = False\n",
    "        self._head_text = []\n",
    "        self._table = False\n",
    "        self._table_text = []\n",
    "        self._img_count = 0\n",
    "        self._anchor_count = 0\n",
    "        self._meta = []\n",
    "        self._link_count = 0\n",
    "        self._li_count = 0\n",
    "        self._style_count = 0\n",
    "        self._script_count = 0\n",
    "        self._div_count = 0\n",
    "        self._iframe_count = 0\n",
    "        self._tag_count = 0\n",
    "        self._anchor_data = []\n",
    "        self._h1_count = 0\n",
    "        self._h2_count = 0\n",
    "        self._h3_count = 0\n",
    "        \n",
    "\n",
    "    def handle_data(self, data):\n",
    "        text = data.strip()\n",
    "        if len(text) > 0:\n",
    "            text = re.sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            self._text.append(text + ' ')\n",
    "            if self._in_title:\n",
    "                self._in_title = False\n",
    "                self._title_text.append(text + ' ')\n",
    "            elif self._anchor:\n",
    "                self._anchor = False\n",
    "                self._anchor_text.append(text + ' ')\n",
    "            elif self._head:\n",
    "                self._head = False\n",
    "                self._head_text.append(text + ' ')\n",
    "            elif self._table:\n",
    "                self._table = False\n",
    "                self._table_text.append(text + ' ')\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self._tag_count += 1\n",
    "        if tag == 'p':\n",
    "            self._text.append('\\n\\n')\n",
    "        elif tag == 'br':\n",
    "            self._text.append('\\n')\n",
    "        elif tag == 'title':\n",
    "            self._in_title = True\n",
    "        elif tag == 'a':\n",
    "            self._anchor = True\n",
    "            self._anchor_count += 1\n",
    "            self._anchor_data.append(dict(attrs))\n",
    "        elif tag == 'head':\n",
    "            self._head = True\n",
    "        elif tag == 'table':\n",
    "            self._table = True\n",
    "        elif tag == 'img':\n",
    "            self._img_count += 1\n",
    "        elif tag == 'meta':\n",
    "            self._meta.append(dict(attrs))\n",
    "        elif tag == 'link':\n",
    "            self._link_count += 1\n",
    "        elif tag == 'li':\n",
    "            self._li_count += 1\n",
    "        elif tag == 'style':\n",
    "            self._style_count += 1\n",
    "        elif tag == 'div':\n",
    "            self._div_count += 1\n",
    "        elif tag == 'script':\n",
    "            self._script_count += 1\n",
    "        elif tag == 'iframe':\n",
    "            self._iframe_count += 1\n",
    "        elif tag == 'h1':\n",
    "            self._h1_count += 1\n",
    "        elif tag == 'h2':\n",
    "            self._h2_count += 1\n",
    "        elif tag == 'h3':\n",
    "            self._h3_count += 1\n",
    "            \n",
    "    def handle_startendtag(self, tag, attrs):\n",
    "        if tag == 'br':\n",
    "            self._text.append('\\n\\n')\n",
    "\n",
    "    def text(self):\n",
    "        return ''.join(self._text).strip()\n",
    "    \n",
    "    def title_text(self):\n",
    "        return ''.join(self._title_text).strip()\n",
    "    \n",
    "    def anchor_text(self):\n",
    "        return ''.join(self._anchor_text).strip()\n",
    "    \n",
    "    def head_text(self):\n",
    "        return ''.join(self._head_text).strip()\n",
    "    \n",
    "    def table_text(self):\n",
    "        return ''.join(self._table_text).strip()\n",
    "    \n",
    "    def meta(self):\n",
    "        return [self._meta, self._anchor_data]\n",
    "    \n",
    "    def tag_counts(self):\n",
    "        return [self._anchor_count, self._img_count, self._link_count, self._li_count,\n",
    "                self._style_count, self._iframe_count, self._div_count, self._script_count,\n",
    "                self._h1_count, self._h2_count, self._h3_count, self._tag_count]\n",
    "    \n",
    "    \n",
    "\n",
    "@convert2unicode\n",
    "def html2text_parser(text):\n",
    "    parser = TextHTMLParser()\n",
    "    parser.feed(text)\n",
    "    return [parser.text(), parser.title_text(), parser.anchor_text(), parser.head_text(), \n",
    "            parser.table_text()] + parser.meta() + parser.tag_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "html2text = html2text_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@convert2lower\n",
    "@convert2unicode\n",
    "def easy_tokenizer(text):\n",
    "    word = unicode()\n",
    "    for symbol in text:\n",
    "        if symbol.isalnum(): word += symbol\n",
    "        elif word:\n",
    "            yield word\n",
    "            word = unicode()\n",
    "    if word: yield word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def html2word(raw_html, to_text=html2text, tokenizer=easy_tokenizer):\n",
    "    text, title_text, anchor_text, head_text, table_text, meta, anchor_data, anchor, img, \\\n",
    "    link, li, style, iframe, div, script, h1, h2, h3, tag = to_text(raw_html)\n",
    "    return [list(tokenizer(text.lower())), list(tokenizer(title_text.lower())), list(tokenizer(anchor_text.lower())), \n",
    "            list(tokenizer(head_text.lower())), list(tokenizer(table_text.lower())), \n",
    "            meta, anchor_data, anchor, img, link, li, style, iframe, div, script, h1, h2, h3, tag]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_dict = Counter()\n",
    "not_spam_dict = Counter()\n",
    "spam_url = Counter()\n",
    "not_spam_url = Counter()\n",
    "Docs_train = []\n",
    "Docs_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "stop_words = stopwords.words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_docs_train(html_data, mark):\n",
    "    packed = html2word(html_data)\n",
    "    words = list(packed[0])\n",
    "    anchor_data = packed[6]\n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    text = text.encode('utf-8')\n",
    "    Docs_train.append(text)\n",
    "    link_titles = []\n",
    "    for anchor_dict in anchor_data:\n",
    "        if 'href' in anchor_dict:\n",
    "            url = anchor_dict['href']\n",
    "        if 'title' in anchor_dict and anchor_dict['title'] is not None:\n",
    "            link_titles.extend(filter(len, anchor_dict['title'].split()))\n",
    "    \n",
    "    if mark:\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                spam_dict[word] += 1\n",
    "        for word in link_titles:\n",
    "            spam_url[word] += 1\n",
    "    else:\n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                not_spam_dict[word] += 1\n",
    "        for word in link_titles:\n",
    "            not_spam_url[word] += 1\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_docs_test(html_data, mark=None):\n",
    "    packed = html2word(html_data)\n",
    "    words = list(packed[0])\n",
    "    text = ' '.join(words)\n",
    "    text = text.encode('utf-8')\n",
    "    Docs_test.append(text)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_dicts(input_file_name, func):    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            if int(parts[1]) == -1:\n",
    "                mark = None\n",
    "            else:\n",
    "                mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            func(html_data, mark)          \n",
    "                \n",
    "        trace(i, 1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:28:07 INFO:Complete items 00000\n",
      "15:28:40 INFO:Complete items 01000\n",
      "15:29:07 INFO:Complete items 02000\n",
      "15:29:36 INFO:Complete items 03000\n",
      "15:30:04 INFO:Complete items 04000\n",
      "15:30:34 INFO:Complete items 05000\n",
      "15:31:02 INFO:Complete items 06000\n",
      "15:31:33 INFO:Complete items 07000\n",
      "15:31:34 INFO:Complete items 07043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 26s, sys: 640 ms, total: 3min 27s\n",
      "Wall time: 3min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab.csv.gz'\n",
    "csv_to_dicts(TRAIN_DATA_FILE, load_docs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:35:58 INFO:Complete items 00000\n",
      "15:36:28 INFO:Complete items 01000\n",
      "15:36:55 INFO:Complete items 02000\n",
      "15:37:19 INFO:Complete items 03000\n",
      "15:37:42 INFO:Complete items 04000\n",
      "15:38:03 INFO:Complete items 05000\n",
      "15:38:29 INFO:Complete items 06000\n",
      "15:38:51 INFO:Complete items 07000\n",
      "15:39:13 INFO:Complete items 08000\n",
      "15:39:37 INFO:Complete items 09000\n",
      "15:40:08 INFO:Complete items 10000\n",
      "15:40:30 INFO:Complete items 11000\n",
      "15:40:52 INFO:Complete items 12000\n",
      "15:41:14 INFO:Complete items 13000\n",
      "15:41:38 INFO:Complete items 14000\n",
      "15:42:02 INFO:Complete items 15000\n",
      "15:42:29 INFO:Complete items 16000\n",
      "15:42:30 INFO:Complete items 16038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 32s, sys: 1.23 s, total: 6min 33s\n",
      "Wall time: 6min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_DATA_FILE  = 'kaggle_test_data_tab.csv.gz'\n",
    "csv_to_dicts(TEST_DATA_FILE, load_docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(spam_dict)\n",
    "print len(not_spam_dict)\n",
    "print len(spam_url)\n",
    "print len(Docs_train)\n",
    "print len(Docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(spam_dict, \"spam_dict\")\n",
    "save_obj(not_spam_dict, \"not_spam_dict\")\n",
    "save_obj(spam_url, \"spam_url\")\n",
    "save_obj(not_spam_url, \"not_spam_url\")\n",
    "save_obj(Docs_train, \"Docs_train\")\n",
    "save_obj(Docs_test, \"Docs_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_dict = load_obj(\"spam_dict\")\n",
    "not_spam_dict = load_obj(\"not_spam_dict\")\n",
    "spam_url = load_obj(\"spam_url\")\n",
    "not_spam_url = load_obj(\"not_spam_url\")\n",
    "Docs_train = load_obj(\"Docs_train\")\n",
    "Docs_test = load_obj(\"Docs_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350855\n",
      "448424\n",
      "9302\n",
      "7044\n",
      "16039\n"
     ]
    }
   ],
   "source": [
    "print len(spam_dict)\n",
    "print len(not_spam_dict)\n",
    "print len(spam_url)\n",
    "print len(Docs_train)\n",
    "print len(Docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21450.5082961\n"
     ]
    }
   ],
   "source": [
    "avg_l = np.mean(map(len, Docs_train + Docs_test))\n",
    "print avg_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    }
   ],
   "source": [
    "most_common_spam_200 = dict(spam_dict.most_common(200))\n",
    "most_common_not_spam_200 = dict(not_spam_dict.most_common(200))\n",
    "most_common_spam = dict()\n",
    "for key in most_common_spam_200:\n",
    "    if key not in stop_words and key not in most_common_not_spam_200:\n",
    "        most_common_spam[stemmer.stem(key)] = most_common_spam_200[key] / len(Docs_train)\n",
    "print len(most_common_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=[u'\\u0438', u'\\u0432', u'\\u0432\\u043e', u'\\u043d\\u0435', u'\\u0447\\u0442\\u043e', u'\\u043e\\u043d', u'\\u043d\\u0430', u'\\u044f', u'\\u0441', u'\\u0441\\u043e', u'\\u043a\\u0430\\u043a', u'\\u0430', u'\\u0442\\u043e', u'\\u0432\\u0441\\u0435', u'\\u043e\\u043d\\u0430', u'\\u0442\\u0430\\u043a', u'\\u0435\\u0433\\u...043a\\u043e\\u043d\\u0435\\u0447\\u043d\\u043e', u'\\u0432\\u0441\\u044e', u'\\u043c\\u0435\\u0436\\u0434\\u0443'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "vectorizer.fit(Docs_train + Docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def safe_divide(a, b):\n",
    "    if a == 0: return 0.0\n",
    "    elif b == 0: return 0.0\n",
    "    else: return a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def url_features(url):\n",
    "    size = 6\n",
    "    Features = [0] * size\n",
    "    if not isinstance(url, str):\n",
    "        return Features\n",
    "    url = re.sub(r\".+://\", r\"\", url)\n",
    "    url = re.sub(r\"/$\", r\"\", url)\n",
    "    url = re.sub(r\"\\n\", r\"\", url)\n",
    "    lst_split = re.split(r\"\\?\" , urllib.unquote(unicode(url)))\n",
    "    #убираем имя хоста\n",
    "    path = re.split(r\"/\" , lst_split[0], maxsplit=1)\n",
    "    params = []\n",
    "    if len(lst_split) == 2:\n",
    "        params = re.split(r\"&\" , lst_split[1])\n",
    "    idx = 0\n",
    "    Features[idx] = len(params)\n",
    "    idx += 1\n",
    "    if len(path) == 2:\n",
    "        path = path[1]\n",
    "        Features[idx] = len(re.findall(r'/', path)) + 1\n",
    "        idx += 1\n",
    "        segments = re.split(r\"/\", path)\n",
    "        for segment in segments:\n",
    "            Features[idx] = len(segment)\n",
    "            idx += 1\n",
    "            ext = re.search(r\"(\\.)([^\\.]+$)\" , segment)\n",
    "            Features[idx] = int(bool(ext))\n",
    "            idx += 1\n",
    "            if idx == len(Features):\n",
    "                break\n",
    "    \n",
    "    return Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#здесь может получиться много нулевых фичей, но случайный лес позаботится об отборе нужных фичей\n",
    "def statistics(sample):\n",
    "    size = len(sample)\n",
    "    if size == 0:\n",
    "        return [0.0] * 17\n",
    "    mean = np.mean(sample)\n",
    "    maximum = max(sample)\n",
    "    minimum = min(sample)\n",
    "    _range = maximum - minimum\n",
    "    deviation = np.std(sample)\n",
    "    median = np.median(sample)\n",
    "    iqr = stats.iqr(sample)\n",
    "    mode = stats.mode(sample)[0][0]\n",
    "    mode_amplitude = safe_divide(sample.count(mode), size * 100.0)\n",
    "    volt_index = safe_divide(mode_amplitude, 2.0 * mode * _range)\n",
    "    veg_bal_ind = safe_divide(mode_amplitude, _range)\n",
    "    veg_rithm = safe_divide(1.0, mode * _range)\n",
    "    adequacy = safe_divide(mode_amplitude, mode)\n",
    "    hmean = stats.hmean(filter(lambda x: x > 0.0, sample))\n",
    "    gmean = stats.gmean(sample)\n",
    "    kurtosis = stats.kurtosis(sample)\n",
    "    skew = stats.skew(sample)\n",
    "    \n",
    "    return [mean, maximum, minimum, _range, deviation, median, iqr, mode, mode_amplitude,\n",
    "            volt_index, veg_bal_ind, veg_rithm, adequacy, hmean, gmean, kurtosis, skew]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calc_features(url, html_data, mark=None):\n",
    "    words, title_words, anchor_words, head_words, table_words, meta, anchor_data, anchor_count, \\\n",
    "    img_count, link_count, li_count, style_count, iframe_count, div_count, script_count, \\\n",
    "    h1_count, h2_count, h3_count, tag_count = html2word(html_data)\n",
    "    \n",
    "    words_num = len(words)\n",
    "    title_words_num = len(title_words)\n",
    "    anchor_words_num = len(anchor_words)\n",
    "    head_words_num = len(head_words)\n",
    "    table_words_num = len(table_words)\n",
    "    meta_count = len(meta)\n",
    "    \n",
    "    words_len = map(len, words)\n",
    "    title_words_len = map(len, title_words)\n",
    "    anchor_words_len = map(len, anchor_words)\n",
    "    head_words_len = map(len, head_words)\n",
    "    table_words_len = map(len, table_words)\n",
    "    \n",
    "    stats = (statistics(words_len) + statistics(title_words_len) + statistics(anchor_words_len)\n",
    "             + statistics(head_words_len) + statistics(table_words_len))\n",
    "    \n",
    "    compression_level = 0\n",
    "    text = ' '.join(words)\n",
    "    text = text.encode('utf-8')\n",
    "    compressed = zlib.compress(text)\n",
    "    compression_level = 1.0 - safe_divide(len(compressed), len(text))\n",
    "    \n",
    "    keywords = []\n",
    "    for meta_dict in meta:\n",
    "        if u'name' in meta_dict and meta_dict[u'name'] == 'keywords':\n",
    "            if u'content' in meta_dict:\n",
    "                keywords = filter(len, re.split(r\",| \", meta_dict[u'content'].lower()))\n",
    "        elif u'name' in meta_dict and meta_dict[u'name'] == 'description':\n",
    "            if u'content' in meta_dict:\n",
    "                keywords = filter(len, re.split(r\",| \", meta_dict[u'content'].lower()))\n",
    "    \n",
    "    keywords_freq = [0.0]* len(keywords)\n",
    "    keywords_ratio = [0.0]* len(keywords)\n",
    "    keywords_w = [0.0]* len(keywords)\n",
    "    k1 = 0.0\n",
    "    b = 0.0\n",
    "    for idx, keyword in enumerate(keywords):\n",
    "        keywords_freq[idx] = words.count(keyword)\n",
    "        if keyword in vectorizer.vocabulary_:\n",
    "            keywords_w[idx] = (k1 + 1) * keywords_freq[idx] / (k1 + ((1 - b) + b * (len(text)/avg_l))) \\\n",
    "             * vectorizer.idf_[vectorizer.vocabulary_[keyword]]\n",
    "            keywords_ratio[idx] = keywords_w[idx] / words_num\n",
    "       \n",
    "    stats.extend(statistics(keywords_freq))\n",
    "    stats.extend(statistics(keywords_ratio)) \n",
    "    stats.extend(statistics(keywords_w))\n",
    "    \n",
    "    words_freq = Counter()\n",
    "    for word in words:\n",
    "        words_freq[word] += 1\n",
    "    stats.extend(statistics(words_freq.values()))\n",
    "    \n",
    "    link_features = []\n",
    "    link_titles = []\n",
    "    for anchor_dict in anchor_data:\n",
    "        if 'href' in anchor_dict:\n",
    "            url = anchor_dict['href']\n",
    "            link_features.append(url_features(url))\n",
    "        if 'title' in anchor_dict and anchor_dict['title'] is not None:\n",
    "            link_titles.extend(filter(len, anchor_dict['title'].split()))\n",
    "    if link_features:\n",
    "        stats.extend(statistics(np.mean(np.array(link_features), axis=0).tolist()))\n",
    "    else:\n",
    "        stats.extend(statistics([]))\n",
    "    \n",
    "    bayes = 0\n",
    "    bayes_url = 0\n",
    "    most_common_counter = {key:0 for key in most_common_spam}\n",
    "    \n",
    "    for word in words:\n",
    "        stemmed = stemmer.stem(word)\n",
    "        if stemmed in most_common_counter:\n",
    "            most_common_counter[stemmed] += 1\n",
    "        \n",
    "        if word in spam_dict and word in not_spam_dict:\n",
    "            bayes += np.log(spam_dict[word] / not_spam_dict[word])\n",
    "        elif word in spam_dict and word not in not_spam_dict:\n",
    "            bayes += np.log(spam_dict[word])\n",
    "        elif word not in spam_dict and word in not_spam_dict:\n",
    "            bayes -= np.log(not_spam_dict[word])\n",
    "    bayes /= words_num\n",
    "    \n",
    "    for word in link_titles:\n",
    "        if word in spam_url and word in not_spam_url:\n",
    "            bayes_url += np.log(spam_url[word] / not_spam_url[word])\n",
    "        elif word in spam_url and word not in not_spam_url:\n",
    "            bayes_url += np.log(spam_url[word])\n",
    "        elif word not in spam_url and word in not_spam_url:\n",
    "            bayes_url -= np.log(not_spam_url[word])\n",
    "    bayes_url = safe_divide(bayes_url, len(link_titles))\n",
    "    \n",
    "    for key in most_common_counter:\n",
    "        most_common_counter[key] *= most_common_spam[key]\n",
    "    \n",
    "    most_common = most_common_counter.values()\n",
    "    stats.extend(statistics(most_common))\n",
    "    \n",
    "    return [words_num, title_words_num, anchor_words_num, head_words_num, table_words_num,\n",
    "            img_count, anchor_count, meta_count, link_count, li_count, style_count, iframe_count, div_count, script_count,\n",
    "            compression_level, tag_count, h1_count, h2_count, h3_count,\n",
    "            safe_divide(words_num, tag_count), safe_divide(tag_count, anchor_count + link_count), \n",
    "            safe_divide(words_num, script_count), safe_divide(tag_count, script_count), bayes, bayes_url] \\\n",
    "            + url_features(url) + stats + [sum(most_common)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DocItem = namedtuple('DocItem', ['doc_id', 'is_spam', 'url', 'features'])\n",
    "\n",
    "def load_csv(input_file_name, calc_features_f):    \n",
    "    \"\"\"\n",
    "    Загружаем данные и извлекаем на лету признаки\n",
    "    Сам контент не сохраняется, чтобы уменьшить потребление памяти - чтобы\n",
    "    можно было запускать даже на ноутбуках в классе\n",
    "    \"\"\"\n",
    "    \n",
    "    with gzip.open(input_file_name) if input_file_name.endswith('gz') else open(input_file_name)  as input_file:            \n",
    "        headers = input_file.readline()\n",
    "        \n",
    "        for i, line in enumerate(input_file):\n",
    "            trace(i)\n",
    "            parts = line.strip().split('\\t')\n",
    "            url_id = int(parts[0])                                        \n",
    "            if int(parts[1]) == -1:\n",
    "                mark = None\n",
    "            else:\n",
    "                mark = bool(int(parts[1]))                    \n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "            html_data = base64.b64decode(pageInb64)\n",
    "            features = calc_features_f(url, html_data, mark) \n",
    "            yield DocItem(url_id, mark, url, features)            \n",
    "                \n",
    "        trace(i, 1)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:03:08 INFO:Complete items 00000\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/scipy/stats/stats.py:377: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return size / np.sum(1.0 / a, axis=axis, dtype=dtype)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/scipy/stats/stats.py:305: RuntimeWarning: divide by zero encountered in log\n",
      "  log_a = np.log(np.array(a, dtype=dtype))\n",
      "21:07:26 INFO:Complete items 01000\n",
      "21:10:47 INFO:Complete items 02000\n",
      "21:14:34 INFO:Complete items 03000\n",
      "21:18:25 INFO:Complete items 04000\n",
      "21:22:27 INFO:Complete items 05000\n",
      "21:25:30 INFO:Complete items 06000\n",
      "21:29:25 INFO:Complete items 07000\n",
      "21:29:32 INFO:Complete items 07043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7044\n",
      "CPU times: user 26min 25s, sys: 4.59 s, total: 26min 29s\n",
      "Wall time: 26min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TRAIN_DATA_FILE  = 'kaggle_train_data_tab.csv.gz'\n",
    "train_docs = list(load_csv(TRAIN_DATA_FILE, calc_features))\n",
    "print len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21:29:32 INFO:Complete items 00000\n",
      "21:33:55 INFO:Complete items 01000\n",
      "21:37:34 INFO:Complete items 02000\n",
      "21:41:20 INFO:Complete items 03000\n",
      "21:44:48 INFO:Complete items 04000\n",
      "21:48:01 INFO:Complete items 05000\n",
      "21:51:40 INFO:Complete items 06000\n",
      "21:55:05 INFO:Complete items 07000\n",
      "21:58:51 INFO:Complete items 08000\n",
      "22:02:14 INFO:Complete items 09000\n",
      "22:06:23 INFO:Complete items 10000\n",
      "22:09:42 INFO:Complete items 11000\n",
      "22:13:02 INFO:Complete items 12000\n",
      "22:16:00 INFO:Complete items 13000\n",
      "22:19:35 INFO:Complete items 14000\n",
      "22:23:16 INFO:Complete items 15000\n",
      "22:26:50 INFO:Complete items 16000\n",
      "22:27:03 INFO:Complete items 16038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16039\n",
      "CPU times: user 57min 41s, sys: 10.2 s, total: 57min 51s\n",
      "Wall time: 57min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "TEST_DATA_FILE  = 'kaggle_test_data_tab.csv.gz'\n",
    "test_docs = list(load_csv(TEST_DATA_FILE, calc_features))\n",
    "print len(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_obj(train_docs, \"train_docs\")\n",
    "save_obj(test_docs, \"test_docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.feature_selection import SelectFromModel, SelectKBest\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.base import BaseEstimator\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7044, 219)\n",
      "(7044,)\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for doc in train_docs:\n",
    "    X_train.append(doc.features)\n",
    "    y_train.append(doc[1])\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train).astype(int)\n",
    "print X_train.shape\n",
    "print y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.nan_to_num(X_train)\n",
    "X_norm = scale(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf1 = RandomForestClassifier(400, n_jobs=-1)\n",
    "clf2 = xgb.XGBClassifier(n_estimators = 500, n_jobs=-1)\n",
    "clf3 = lgbm.LGBMClassifier(n_estimators=100, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98453069177\n",
      "0.987508881302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.986087533686\n"
     ]
    }
   ],
   "source": [
    "print np.mean(cross_val_score(clf1, X_train, y_train, cv=7, n_jobs=-1, scoring='f1_weighted'))\n",
    "print np.mean(cross_val_score(clf2, X_train, y_train, cv=7, n_jobs=-1, scoring='f1_weighted'))\n",
    "print np.mean(cross_val_score(clf3, X_train, y_train, cv=7, n_jobs=-1, scoring='f1_weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "/home/jekrus/anaconda2/lib/python2.7/site-packages/lightgbm/basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', colsample_bytree=1.0, learning_rate=0.1,\n",
       "        max_bin=255, max_depth=-1, min_child_samples=20,\n",
       "        min_child_weight=0.001, min_split_gain=0.0, n_estimators=100,\n",
       "        n_jobs=-1, num_leaves=31, objective=None, random_state=None,\n",
       "        reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "        subsample_for_bin=200000, subsample_freq=1)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(X_train, y_train)\n",
    "clf2.fit(X_train, y_train)\n",
    "clf3.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16039, 219)\n"
     ]
    }
   ],
   "source": [
    "X_test = []\n",
    "for doc in test_docs:\n",
    "    X_test.append(doc.features)\n",
    "X_test = np.array(X_test)\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = np.nan_to_num(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result1 = clf1.predict(X_test)\n",
    "result2 = clf2.predict(X_test)\n",
    "result3 = clf3.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = np.zeros(result1.shape[0]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(result1.shape[0]):\n",
    "    if result1[i] + result2[i] == 2 or result1[i] + result3[i] == 2 or result2[i] + result3[i] == 2:\n",
    "        result[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('my_submission.csv' , 'wb') as fout:\n",
    "    writer = csv.writer(fout)\n",
    "    writer.writerow(['Id','Prediction'])\n",
    "    for idx, doc in enumerate(test_docs):\n",
    "        writer.writerow([doc.doc_id, result[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
